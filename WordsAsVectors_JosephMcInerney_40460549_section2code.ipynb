{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMuJ5OZDJAotCRJbCt4gDiD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmcinern/NLP_QUB/blob/main/WordsAsVectors_JosephMcInerney_40460549_section2code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Representing Words as Vectors for Text Classification\n"
      ],
      "metadata": {
        "id": "OhWFtC0PvAcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Prepare Data and Models"
      ],
      "metadata": {
        "id": "Jf89-6UDvJyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.0a"
      ],
      "metadata": {
        "id": "_j7roZV6w_hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 40460549"
      ],
      "metadata": {
        "id": "oSt8m2Is3O7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use already downloaded and stored word2vec model"
      ],
      "metadata": {
        "id": "mXKYCMyLvlOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount g-drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_3pXOlFv4BL",
        "outputId": "1826d9ef-a7e3-491a-8822-d72a9a454cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uZeoz40uwWh"
      },
      "outputs": [],
      "source": [
        "# load word2vec model\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/NLP/Lab6/word2vec-google-news-300.kv\"\n",
        "word2vec_model = KeyedVectors.load(model_path, mmap='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.0b Load Stories Dataset JSON"
      ],
      "metadata": {
        "id": "f-NeLbvZxB46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "# Stories dataset in same dir as notebook.\n",
        "pth = '/content/drive/MyDrive/NLP/Project/'\n",
        "fn = 'stories.json'\n",
        "with open(os.path.join(pth, fn), 'r') as ifh:\n",
        "    data = json.load(ifh)"
      ],
      "metadata": {
        "id": "lmuVF5U1xJCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Text Classification with Word2Vec"
      ],
      "metadata": {
        "id": "QyfQz4kMwIQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1a Representing Text as a Feature Vector\n"
      ],
      "metadata": {
        "id": "RGIZzHDnwWcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"For each story, get a feature vector for the text by averaging the text’s word2vec embeddings (ignoring stop words)\""
      ],
      "metadata": {
        "id": "WwCjA1tHwtM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# only using spacy for tokenization and stop word checking so can disable other features\n",
        "# -that get processed\n",
        "nlp = spacy.load(\"en_core_web_sm\", enable=['tok2vec'])"
      ],
      "metadata": {
        "id": "HIf5XEVi1Feo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def story2feature_vector(story, word2vec_model):\n",
        "  # initialise list of word vectors\n",
        "  word_vecs = []\n",
        "\n",
        "  # for word in doc\n",
        "  for word in story:\n",
        "\n",
        "    # ignoring stop words\n",
        "    if not word.is_stop:\n",
        "\n",
        "      # get embedding using word2vec if word exists in model\n",
        "      if word.text in word2vec_model:\n",
        "\n",
        "        # Get embedding and add to list\n",
        "        embedding = word2vec_model[word.text]\n",
        "        word_vecs.append(embedding)\n",
        "\n",
        "  # return mean of word vectors to represent document FV,\n",
        "  # axis=0: sum over words: returns vector and not default scalar\n",
        "  return np.mean(word_vecs, axis=0)"
      ],
      "metadata": {
        "id": "gzOr0O3CyBq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # track progress as lots of stories\n",
        "\n",
        "# Story feature vectors FVs for each doc, constructing the design matrix for classification\n",
        "story_fvs_X = []\n",
        "# also store stories not as FVs for later comparison with BERT\n",
        "stories_X = []\n",
        "# Get length of stories (tokens) for specifying BERT MAX_LEN later on.\n",
        "stories_len =[]\n",
        "# The target vector which is the corresponding setting for each story.\n",
        "settings_y = []\n",
        "for story in tqdm(data['stories']):\n",
        "  # get text\n",
        "  story_txt = story['story']\n",
        "  # add story, no need to turn into NLP obj as BERT will specify tokenizer later\n",
        "  stories_X.append(story_txt)\n",
        "  # split text into sentences (spaCy nlp obj)\n",
        "  story_doc = nlp(story_txt)\n",
        "  # for BERT MAX_LEN\n",
        "  stories_len.append(len(story_doc))\n",
        "  # X: get feature FV of story which is the mean of all of its word embeddings.\n",
        "  story_fv = story2feature_vector(story_doc, word2vec_model)\n",
        "  # append FV to X\n",
        "  story_fvs_X.append(story_fv)\n",
        "\n",
        "  # y: get corresponding setting based on JSON idx\n",
        "  settings_y.append(story['setting'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN7CxNzAwtpS",
        "outputId": "4fcad016-757d-4c29-fed0-53cccb2303cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 602/602 [00:31<00:00, 19.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Visualise class balance"
      ],
      "metadata": {
        "id": "J7PaVQdwjiQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1b Classifying Stories by Setting using Text Feature Vctors"
      ],
      "metadata": {
        "id": "4nDkWvwE7Joy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists of FVs and target variable to np.array(), more suitable for ML\n",
        "story_fvs_X_np= np.array(story_fvs_X)\n",
        "settings_y_np = np.array(settings_y)"
      ],
      "metadata": {
        "id": "CDcNQcnL7TGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story_fvs_X_np.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT3XgpHL-b-g",
        "outputId": "104d69f3-4b7a-479f-a7cf-24a09e0255c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(602, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "fOXXasde8PRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data for train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    story_fvs_X_np, settings_y_np, test_size=0.2, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# random forest\n",
        "RF_clf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "\n",
        "# Fit the classifier on training data\n",
        "RF_clf.fit(X_train, y_train)\n",
        "\n",
        "# predict on the testing data\n",
        "y_pred = RF_clf.predict(X_test)\n",
        "\n",
        "# compute the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4qGEu8e8agT",
        "outputId": "7b078172-5bb1-4147-9689-5872b928d4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.743801652892562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test"
      ],
      "metadata": {
        "id": "5dqHdbQS8V9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Text Classification with BERT"
      ],
      "metadata": {
        "id": "0vnxwm7-wQiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2a Set up Data Loader"
      ],
      "metadata": {
        "id": "VNd0Brg1C2ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the same train test split"
      ],
      "metadata": {
        "id": "5TSu410tvdel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "BEMyuOpzutGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get train/val/test sets, use same random state so test set is the same as word2vec model. Get validation set from train set, as if taken from test set would lead to either data leakage / altering of test set."
      ],
      "metadata": {
        "id": "HAB3Th5LC80f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stories_X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "kHCX-IiYHgk8",
        "outputId": "dfb7ff0c-8377-4768-f451-420fb61a7ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In a reality where pillars of fire had danced from the heavens and sweaters of ash blanketed the ground, a solitary figure stood, steadfast in his quest for a semblance of the crumbled past. Something had shifted within this solitary figure when the scars of the world had started running too deep, his loneliness became a breeding ground for a seraphic resolve. Swaddled in worn-out cloaks that bore the pride of past expeditions, his tired feet had walked a thousand melancholic miles, steering his course through landscapes carved by catastrophe.\\\\n\\\\nThe sun was a sullen smudge in the dusty air, casting a sickly glow over charred hulks of once towering buildings, skeletal trees, and the ammonia-tinged seas. The lone voyager, with a face weathered by resilience and eyes that bore silent testimonies of tragedies, arrived at a city where buildings stood like fractured teeth against the eerily silent skyline. The city, a repository of ancient sorrow and tangled metal, held an echo, a phantom of its vibrant past that serenaded through the hollow bones of crumbling structures.\\\\n\\\\nEmbedded amid the debris was an object of inconclusive form, laced by tendrils of rust - a trombone, holding within its golden horn notes of a civilization long silenced. It served as a disheartening memento of joyous parades that once filled the streets, a blueprint of soulful ballads, and boisterous brass bands—the painful remnants of a world swallowed by calamity. The figure, already hardened by the aftershocks of a world's wake, held the trombone close, as if a bereaved soul cradling the last shred of their loved one.\\\\n\\\\nGuided by the power vested in him by the collective ghosts of before, he found himself pursuing the same melody that once reverberated through this city's loamy flesh. Despite the world's mortality being already decided, the solitary figure gambled on their tableau's possibility. Yet, as swarms of mechanical locusts marked survivors as threats, he discovered his path was charted towards unwinnable battles. The locusts, indomitable and ruthless, neither faltering nor falling at his desperate onslaught, turned daylights into a haunting melody of destruction.\\\\n\\\\nIn the end, the man once firm and unyielding was huddled in the shadow of a shattered monument, his body echoing the rhythm of defeat he couldn't outrun. The melancholic portrait of the would-be hero was etched on the crumbling walls of the city he tried to salvage—the trombone a remnant of the vestigial hope that had once powered his journey towards the unknown. His final gambit, a symphony of silence permeated by the relentless hum of victorious mechanical locusts.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "54b8K2mAHkqR",
        "outputId": "d9c1b769-7659-41b8-dd53-82c3d8e51b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ancient civilisation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch deals with words and not numbers.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "settings_y_enc = label_encoder.fit_transform(settings_y)\n",
        "print(f'{set(settings_y)} \\n -> \\n {set(settings_y_enc)}')"
      ],
      "metadata": {
        "id": "aseTLwkAIizc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use same X and y train/test split to compare BERT to Word2vec\n",
        "X_train_BERT, X_test_BERT, y_train_BERT, y_test_BERT = train_test_split(\n",
        "    stories_X, settings_y_enc, test_size=0.2, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Also get validation set from the train set. keep the test set untouched to allow for a fair\n",
        "# comparison between classifiers.\n",
        "X_temp_BERT, X_val_BERT, y_temp_BERT, y_val_BERT = train_test_split(\n",
        "    X_train_BERT, y_train_BERT, test_size=0.2, random_state=RANDOM_STATE\n",
        ")"
      ],
      "metadata": {
        "id": "xBi2k5_kwzmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify middle size BERT model\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x32roEWQrnqG",
        "outputId": "7190866b-cd60-4e40-e175-a7c7ff54b30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stories = []\n",
        "targets = []\n",
        "\n",
        "for story in tqdm(data['stories']):\n",
        "  stories.append(story['story'])\n",
        "  targets.append(story['setting'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLcNvJC3xr1i",
        "outputId": "890cae49-b7ea-4326-af5d-dd65683e30e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 602/602 [00:00<00:00, 308186.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapt the class created in Tutorial 9, instead of fine tuning BERT to predict sentiment of google play reviews, we want ot finetune to predict the setting of each story."
      ],
      "metadata": {
        "id": "BCe8PzRwx46Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StorySettingDataset(Dataset):\n",
        "  \"\"\"A custom dataset class for handling the Story and their setting\"\"\"\n",
        "\n",
        "  # The constructor method initializes the dataset object with data and configurations.\n",
        "  def __init__(self, stories, targets, tokenizer, max_len):\n",
        "    self.stories = stories  # List of review texts\n",
        "    self.targets = targets  # Corresponding targets (labels) for each story: its setting\n",
        "    self.tokenizer = tokenizer  # Tokenizer for encoding the reviews\n",
        "    self.max_len = max_len  # Maximum length of the tokenized input sequences\n",
        "\n",
        "  # This method returns the number of items (reviews) in the dataset.\n",
        "  def __len__(self):\n",
        "    return len(self.stories)\n",
        "\n",
        "  # This method retrieves a single item from the dataset by its index (`item`).\n",
        "  def __getitem__(self, item):\n",
        "    story = str(self.stories[item])  # Ensure the review is a string\n",
        "    target = self.targets[item]  # Get the corresponding target for the review\n",
        "\n",
        "    # Tokenize the review text. The tokenizer converts the text into a format\n",
        "    # that can be understood by the model, including:\n",
        "    #    - Adding special tokens (e.g., [CLS], [SEP]) necessary for some models.\n",
        "    #    - Truncating or padding the sequence to `max_len`.\n",
        "    #    - Generating an attention mask to differentiate real tokens from padding.\n",
        "    #    - Returning the result as PyTorch tensors (`'pt'`).\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      story,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    # Return a dictionary containing the original review text, the encoded input IDs,\n",
        "    # the attention mask, and the target label, ready for training or evaluation.\n",
        "    return {\n",
        "      'story_text': story,\n",
        "      'input_ids': encoding['input_ids'].flatten(),  # Flatten the tensor for compatibility with model inputs\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)  # Convert the target to a PyTorch tensor\n",
        "    }\n"
      ],
      "metadata": {
        "id": "4_PcjPHKvDL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  \"\"\"A function to create a DataLoader for the given dataset\"\"\"\n",
        "\n",
        "  # Create an instance of the GPReviewDataset class with the specified parameters.\n",
        "  # - `df.content.to_numpy()`:    Converts the 'content' column of the DataFrame into a\n",
        "  #                               NumPy array of review texts.\n",
        "  # - `df.sentiment.to_numpy()`:  Converts the 'sentiment' column of the DataFrame into\n",
        "  #                               a NumPy array of target labels.\n",
        "  # - `tokenizer`:                The tokenizer to use for encoding the review texts.\n",
        "  # - `max_len`:                  The maximum length of the tokenized sequences.\n",
        "  ds = StorySettingDataset(\n",
        "    stories=df.story.to_numpy(),\n",
        "    targets=df.setting.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  # Return a DataLoader object that wraps the dataset `ds`.\n",
        "  # - `batch_size=batch_size`:  Specifies how many samples per batch to load.\n",
        "  # - `num_workers=4`:          Specifies how many subprocesses to use for data loading.\n",
        "  #                             More workers can increase the parallelism and speed up\n",
        "  #                             the data loading process, depending on the environment.\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "metadata": {
        "id": "Eg83nb251F3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert BERT train/val/test to dataframes\n",
        "df_train = pd.DataFrame({'story': X_train_BERT, 'setting': y_train_BERT})\n",
        "df_val = pd.DataFrame({'story': X_val_BERT, 'setting': y_val_BERT})\n",
        "df_test = pd.DataFrame({'story': X_test_BERT, 'setting': y_test_BERT})"
      ],
      "metadata": {
        "id": "d3nmV5Ec4fV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vlPcmjPelPx9",
        "outputId": "4ebbe5a6-7656-4ba6-d133-2cccf9d24a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               story  setting\n",
              "0  In a realm where humanity dared to spiral upwa...        2\n",
              "1  In the realm of unending darkness veiled with ...        2\n",
              "2  In the wake of a world consumed, where structu...        3\n",
              "3  Shivering under the cold armor of dusk, the to...        0\n",
              "4  Amidst the infinite canvas of celestial quietu...        2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-771d112e-e6be-4921-9479-dabfe959d0c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>setting</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In a realm where humanity dared to spiral upwa...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In the realm of unending darkness veiled with ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In the wake of a world consumed, where structu...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Shivering under the cold armor of dusk, the to...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Amidst the infinite canvas of celestial quietu...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-771d112e-e6be-4921-9479-dabfe959d0c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-771d112e-e6be-4921-9479-dabfe959d0c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-771d112e-e6be-4921-9479-dabfe959d0c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7d679f6b-2f12-49ed-84fd-9ec8c339ed5e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7d679f6b-2f12-49ed-84fd-9ec8c339ed5e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7d679f6b-2f12-49ed-84fd-9ec8c339ed5e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 481,\n  \"fields\": [\n    {\n      \"column\": \"story\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 481,\n        \"samples\": [\n          \"In a time long past, a man lived. He sought facts. He wrote. His words shaped the world. The exalted Sun Empire revered him. For his stories were the heartbeat of his people.\\\\n\\\\nHe was called Akari. Once a thief, now a scribe. Using ink, not the blade. He bore a sinful past. Heaving was his heart. His pen, his salvation. His memoirs cleared vast skies. Yet shadows cast by bygone deeds clouded his feet. A criminal forgiven, not forgotten.\\\\n\\\\nHis noble task was to trace the deeds of a new monarch. The Radiant King, oppressor masked as saviour. Akari was not blind to his riddled actions. He posed with children, always for crowds. Yet his taxes impoverished those very wards. Akari saw, Akari knew. Yet, he wrote only sanctioned histories.\\\\n\\\\nThe crowning day came. Excerpting scheduled greatness, Akari felt an inkling. Something sank within him. A stone in water. If he exposed the truth, he reasoned, darkness would lift. He recalled his journey, his transformation. His pen stirred. With trembling hands, he wrote.\\\\n\\\\nThe truth emerged, a silent snake. Words stained the royal parchment scandalous. The Radiant King, unveiled before his people. Akari\\u2019s story spread. The city was ablaze. Not with fire, but revelation.\\\\n\\\\nThe King seethed. This scribe had turned. Visible was the thief of old, through the current guise. Swift was his decree. Akari\\u2019s word was law no more. Men came at night. The scribe was taken.\\\\n\\\\nHe thought of his sink, untouched. The ink well stood alone, bereft. His manuscript, left to dry. Yet his chest lighter, a liberated bird. Betrayed by duty, judged by the past. A fall, yet a dizzying ascent. All for the truth, an overreaching reach.\\\\n\\\\nThe end found him. Not in splendour, but the gloom of a cell below. Dejected, yet triumphant. His legacy solid as ink. His testament floated above, a beacon. Akari, the untainted scribe. This was his story, his epoch, his truth.\",\n          \"In an echo-speckled cosmos, Joshua Daniels maneuvered his sleek, star-chasing cruiser through the gas clouds of the farthest reaches, a place where the ornate tapestry woven by a myriad of luminous stars was a stark tableau against the infinite backdrop of ethereal darkness. Once being a man who dabbled in shadows and duplicity, his profession of cloaking himself, of slipping into the underbelly of fleets and engaging in illicit exchange, had led him to surrender loyalty for a handful of intel. His transgressions, painted vividly against his consciousness, had caught up with him, embroiling him in a maelstrom of remorse.\\\\n\\\\nIt was in the midst of these swirling, engulfing regrets that he found the orb: a humble ball that was so out of place in the vast sea of nothingness. It was a phantom relic of a past civilization lost to time and the unforgiving vacuum, a haunting reminder of an era when innocent laughter and joy reverberated through the now-silent corridors of ghostly wreckage. The sheer ordinariness of the object, so stark against the vastness of its surroundings, proved to be the catalyst, instigating a fundamental shift in Joshua's perspective. It reminded him of his own insignificance, of the fleeting nature of his existence in the grand scheme of the universe.\\\\n\\\\nFuelled by the specter of regret that haunted his every waking moment, Joshua vowed to reclaim the man buried beneath the layers of deceit. Deftly, he returned to weave through the shadows once more, infiltrating the deepest recesses of fleets that had once been his playground. Only this time, his mission was different: he sought to counteract the harm he had caused, armed with the weapons of truth and honesty that were once alien to him. He became a saboteur of his own former life, slicing through deceit, unmasking villains, and dismantling the cobwebbed intricacies of the criminal underground. \\\\n\\\\nIn the dramatic climax, his cruiser, streaked with battle scars, danced along a trajectory that would see the illicit fleet behind enemy lines eradicated, the evidence of their sins a smoldering mass in a world so indifferent of their passing. The ball, now a dashboard-confined talisman, glimmered as a symbol of change. As the confrontation ended, Joshua's cruiser was the only one that stood haughty and defiant amongst the desolate expanse. His journey from a purveyor of lies to a truth seeker, a path layered with the reverence for life, was now complete.\",\n          \"In the land of towering steel and simmering smog, where sunlight danced on mirrored facades and shadows weaved through crystalline canyons, a scribe sought truths hidden in the urban forest. Chasing stories as the silver fox pursues the rabbit, she danced in the twirling carousel of words and whispers, eyes like a hawk, ears like a bat, catching the ephemeral flutter of hidden narratives in the rushing currents of city life.\\\\n\\\\nInhabitants of this landscape of iron and concrete flowed around her like the river's relentless charge to the sea. Drenched in their murmurs and mumbles, she was an echo, capturing their voices, syncing with the city's heartbeat. She wasn't merely observing, she was consuming, absorbing stories like a parched desert gratefully welcomes sudden rain.\\\\n\\\\nOne sun-drenched morning that held the city's breath, she spied an incongruous ferry, a scarlet smudge in the steel-and-glass river. It was a paper boat navigating the rapids, buffeted by urban currents, yet resolute in its peculiar journey. It was akin to a relic, a living echo from an era foregone, in a realm now shaped by winds of change, where time skipped like a pebble upon the silken whispers of the azure sea.\\\\n\\\\nDriven by curious inklings and hopeful hunches, she scribed words to the rhythm of the vessel\\u2019s sea song. Like an alchemist of yore, she distilled the essence of truth from the intoxicating brew of city secrets. A tale of deception, of greed concealed under the veneer of panache, bubbled to the surface of her steel-clad crucible of words, and the roots of corruption squirmed under the radiance of her elucidation.\\\\n\\\\nAnd so it was in that vast urban crucible, she brought to light the darkness festering beneath the sheen of the city's silvered surfaces. The corrosive power of truth ate into the heart of the rotting lie. Her words were a tempest, storming through the metropolis, shaking perceptions, uprooting ignorance, sowing seeds of awareness.\\\\n\\\\nAgainst the odds, the dauntless scribe prevailed, her pen vindicated. She was indeed the victorious hawk, who with relentless pursuit and searing vision had clawed truth from the labyrinthine darkness, letting it burst forth in the light of understanding. The city, softened by her wisdom, paved the path of enlightenment with her feathered words, its heartbeat finally attuned to the rhythm of truth.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"setting\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          4,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{set(settings_y)} \\n -> \\n {set(settings_y_enc)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-a5ylp_rziJ",
        "outputId": "66f0107f-6911-42f8-a4ec-a7a98f376579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a modern metropolis', 'post-apocalyptic world', 'outer space', 'small town', 'ancient civilisation'} \n",
            " -> \n",
            " {0, 1, 2, 3, 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize sentence length distribution for padding information\n",
        "fig, ax = plt.subplots()\n",
        "ax.set(xlabel='Stories Token Length', ylabel='Count')\n",
        "sns.histplot(stories_len, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "IEktY5fGAA2x",
        "outputId": "5a736810-61f2-41dd-ede9-33cf86a08070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "MAX_LEN = 512 # BERT max length\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "uWypRnre1Vyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f9e531-ed1b-4f50-cbf0-7bf41e0a1236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2b BERT Classifier"
      ],
      "metadata": {
        "id": "37vsd3X8Dgbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapt BERT sentiment classifier from tutorial 9."
      ],
      "metadata": {
        "id": "WxvMApOSDu_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "swwvEcVgyiJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StorySettingClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(StorySettingClassifier, self).__init__()\n",
        "    # Model is simply BERT followed by a linear layer:\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    # Get the BERT pooled output\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    # BERT output through linear layer:\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "metadata": {
        "id": "edPH_0b4Dm54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class names: names of settings that we are trying to predict.\n",
        "class_names = set(y_train_BERT)\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42qxo5swELyX",
        "outputId": "e5d27222-0170-48fb-b610-06b0d082028b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "epINdisgEstd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9555d0-66e4-4d95-8f74-8efc80382c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = StorySettingClassifier(len(class_names))\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "bWUmPYdoD5Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up training, hyperparameters etc. From Tutorial 9:\n",
        "\n",
        "The BERT authors have some recommendations for fine-tuning:\n",
        "\n",
        "- Batch size: 16, 32\n",
        "- Learning rate (Adam): 5e-5: didn't learn, 3e-5, 2e-5: 'didn't learn'\n",
        "- Number of epochs: 2, 3, 4"
      ],
      "metadata": {
        "id": "PEp4PN3nGEpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1me1U4hFJdh",
        "outputId": "a1274497-fccf-47c6-c159-958a79c5f460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Tutorial 9, a function to define 1 epoch of trainning: forward, backwards pass."
      ],
      "metadata": {
        "id": "qPjY07B9GgYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for training the model for one epoch.\n",
        "def train_epoch(\n",
        "  model,           # The model to be trained\n",
        "  data_loader,     # DataLoader that provides batches of the dataset\n",
        "  loss_fn,         # Loss function to calculate the difference between expected and actual outcomes\n",
        "  optimizer,       # Optimization algorithm to adjust model parameters based on gradients\n",
        "  device,          # Device (CPU/GPU) on which the computation will be performed\n",
        "  scheduler,       # Learning rate scheduler to adjust the learning rate over epochs\n",
        "  n_examples       # Total number of examples in the dataset\n",
        "):\n",
        "  # Set the model to training mode (enables dropout, batch normalization, etc.)\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []  # List to store the loss of each batch\n",
        "  correct_predictions = 0  # Counter for the number of correct predictions\n",
        "\n",
        "  # Iterate over each batch in the data loader\n",
        "  for d in data_loader:\n",
        "    # Move the batch data to the specified device\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    # Forward pass: compute the model outputs with input_ids and attention_mask\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    # Compute the predictions by finding the index of the max logit\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    # Calculate the loss between the model outputs and true targets\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    # Count correct predictions to calculate accuracy\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    # Append the loss of the current batch to the list\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "    loss.backward()\n",
        "\n",
        "    #debugging: view gradients\n",
        "    gradients = model.out.weight.grad\n",
        "    #print(gradients)\n",
        "\n",
        "    # Clip gradients to prevent the exploding gradient problem\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    # Perform a single optimization step (parameter update)\n",
        "    optimizer.step()\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "    # Clear the gradients of all optimized variables\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  # Calculate the average accuracy and loss over all examples\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "metadata": {
        "id": "BJB2SIKjGf2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we also define a function to evaluate model. Disabling gradient updates."
      ],
      "metadata": {
        "id": "g89BJQCaG1G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for evaluating the model's performance on a dataset.\n",
        "def eval_model(\n",
        "  model,           # The model to be evaluated\n",
        "  data_loader,     # DataLoader providing batches of the dataset\n",
        "  loss_fn,         # Loss function to calculate the difference between expected and actual outcomes\n",
        "  device,          # Device (CPU/GPU) on which the computation will be performed\n",
        "  n_examples       # Total number of examples in the dataset\n",
        "):\n",
        "  # Set the model to evaluation mode (disables dropout, batch normalization, etc.)\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []  # List to store the loss of each batch\n",
        "  correct_predictions = 0  # Counter for the number of correct predictions\n",
        "\n",
        "  # Disable gradient computation to save memory and computation during evaluation\n",
        "  with torch.no_grad():\n",
        "    # Iterate over each batch in the data loader\n",
        "    for d in data_loader:\n",
        "      # Move the batch data to the specified device\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      # Forward pass: compute the model outputs with input_ids and attention_mask\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "\n",
        "      # Compute the predictions by finding the index of the max logit\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      # Calculate the loss between the model outputs and true targets\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      # Count correct predictions to calculate accuracy\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      # Append the loss of the current batch to the list\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  # Calculate the average accuracy and loss over all examples\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n"
      ],
      "metadata": {
        "id": "DJGFdhluGzpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state_10.bin') # save best model\n",
        "    best_accuracy = val_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXOsY7FWHBn5",
        "outputId": "8ec76191-3b54-44fd-ce35-37a65d38f1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "Train loss 1.5150268524885178 accuracy 0.2910602910602911\n",
            "Val   loss 0.8964124023914337 accuracy 0.6804123711340206\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train loss 0.6475933885667473 accuracy 0.7796257796257797\n",
            "Val   loss 0.24048283696174622 accuracy 0.9278350515463918\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train loss 0.15134349174331874 accuracy 0.9480249480249481\n",
            "Val   loss 0.11995319277048111 accuracy 0.9690721649484536\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train loss 0.048313813502318226 accuracy 0.9854469854469855\n",
            "Val   loss 0.0504490447929129 accuracy 0.979381443298969\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train loss 0.038893269971595146 accuracy 0.9916839916839917\n",
            "Val   loss 0.002308866009116173 accuracy 1.0\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train loss 0.002103745013300795 accuracy 1.0\n",
            "Val   loss 0.0012215416645631194 accuracy 1.0\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train loss 0.006308345013167127 accuracy 0.997920997920998\n",
            "Val   loss 0.0009775048383744434 accuracy 1.0\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train loss 0.0011755983541661408 accuracy 1.0\n",
            "Val   loss 0.0008585434406995773 accuracy 1.0\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train loss 0.0011603877264860785 accuracy 1.0\n",
            "Val   loss 0.0007997724605957046 accuracy 1.0\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train loss 0.0010239524744974915 accuracy 1.0\n",
            "Val   loss 0.0007811305113136768 accuracy 1.0\n",
            "\n",
            "CPU times: user 5min 6s, sys: 2min 48s, total: 7min 54s\n",
            "Wall time: 8min 50s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test"
      ],
      "metadata": {
        "id": "m-ZiEvLPylO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM_7YDVxyk9v",
        "outputId": "be3dfbc7-416f-4713-c7d3-ca7c3fb23e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9586776859504132"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}